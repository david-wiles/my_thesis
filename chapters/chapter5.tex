\chapter{KVM Host Considerations}

A machine used as a host for KVM virtual machines should be setup using its own set of guidelines to ensure that the
attack surface between VM's or a VM and the host is minimized.

\section {Overview of KVM}

\subsection{Guidelines and Considerations}

The first step to setting up a server which will host KVM guests is to ensure that general guidelines for linux machines
have been followed. A list of common issues can be found in chapter three of this paper. However, there are some
additional considerations when KVM is in use, especially if the code on the guest should not be trusted such as in a
public cloud environment. Most of the additional considerations are focused on the additional devices which may be used
with a VM as well as IO access from the guest to the external network.

KVM allows for the guest's CPU instructions to be sent directly to the processor if hardware virtualization is supported
by the CPU. This drastically reduces the attack surface and allows us to focus on the components enabling communication between a guest
and any external device or network. The guidelines outlined by Red Hat, Inc. are a good starting place for
determining the most secure system configuration~\cite{redhatcustomerservice}.

\subsection{KVM Performance}

Virtualization today relies heavily on KVM in the linux kernel. Most cloud providers will use this software in order
to create and manage virtual machines, although they may use proprietary software to manage the virtualization frontend.
If the CPU has hardware virtualization enabled then KVM uses hardware virtualization to accomplish
the virtualization of an operating system. This has the benefit of separating the entire guest OS from the host OS
without much of the performance drawbacks of software virtualization. However, it is important to note that even with
hardware acceleration KVM is less performant than linux containers, as shown by Chae, Lee, and Lee~\cite{ChaeMinSu2017Apco}.
Therefore, a KVM-based environment should be chosen for the security benefits whenever the performance drawback is
acceptable.

On a machine hosting VM's, special attention must be paid to the IO operations allowed by the guest machines since this
is the only part of the VM which must be processed by the host OS. Default settings may be acceptable depending on the
virtual machine monitor used but should still be verified by the administrator.

\subsection {Comparision to Container-Based Environments}

Use of a virtual machines for a shared-tenant environment like a public cloud is preferable to a container-based
environment. Albakri, Shanmugam, Samy, Idris, and Ahmed outline several risk assesment considerations for an
organization looking to move assets to a public cloud. They found that the responsibility for security in a cloud
computing environment is shifted to a the cloud provider in a public cloud, and an organization looking to shift
resources to this environment should take this into consideration when evaluating security risks~\cite{AlbakriSameerHasan2014Sraf}.
Due to this, a cloud provider looking to sell its resources should ensure that all possible risks of attacks between
VM's have been mitigated.

\subsection {Overview of issues}

\begin{center}
    \begin{tabular}{||p{0.3\textwidth} p{0.3\textwidth} p{0.3\textwidth}||}
        \hline
        Issue & Attack Type & Affected Components \\ [0.5ex]
        \hline\hline
        Libvirt audits & Intrusion detection & General Considerations \\
        \hline
        Use partitions of physical storage & Privilege escalation & File System \\
        \hline
        Encrypt network storage & Infoleak & File System \\
        \hline
        Disable unused external devices & Remote code execution & Device drivers \\
        \hline
        Untrusted VM Images & InfoLeak, RCE & Network \\
        \hline
        Non-isolated guest networking & Infoleak & Networking \\ [0.5ex]
        \hline
    \end{tabular}
\end{center}

\section{KVM Host Considerations}

\subsection{Set Up libvirt Audits}

Detailed log files are essential to investigating errors which occur on production systems, but can also be helpful when
investigating security issues. As recommended by RedHat, usage of auvirt can help by recording all events that happen
during operation. In public cloud environments, this should be combined with logging of events relating to the user's
use of the system configuration. For example, any changes to networking should be logged, which could then be used to
analyze the VM's events to identify anomalies.

We can check if audits are enabled on the system by running

\begin{lstlisting}[style=AMMA, language=bash]
sudo cat /etc/libvirt/libvirtd.conf | grep 'audit_level'
\end{lstlisting}

We expect the level to be set to 1 or 2. If this is not set, we should edit the file and restart the libvirtd service.

\subsection {Securing Physical Storage Devices}
A guest VM with access to an entire storage device may be able to attack the host or other guests by rewriting volume
labels on the device. Unless the system requirements dictate that a guest should access an entire physical device, this
should be disallowed. There are a large number of possible file system types allowed with KVM but the only one that
could cause issues on the host itself is access to an entire disk. If access to a physical drive is required, we should
ensure that it is done through a partition instead of the device. Additionally, we should make sure that the host
machine does not use file system labels to identify the file systems in the fstab file.

It is more secure to grant a guest access to a partition instead of an entire device. A partition
can be resized or moved as needed, something not possible with a single drive. Additionally, the storage device can be
shared between many guests and partitions will be cleared and reused as needed without affecting other partitions on the
same device.

We can check if the guest OS's storage settings by using virsh. We want to check that the guests do not have access to
entire devices and the host is not using file system labels in the fstab file.

\begin{lstlisting}[style=AMMA, language=bash]
echo "Hello, World"
\end{lstlisting}

If we detect that there is a vulnerable storage device connected, we will want to create a new storage device and
transfer the existing data to it. The data transfer could be done in a number of different ways, but the new storage
device can be set up using

\begin{lstlisting}[style=AMMA, language=bash]
echo "Hello, World"
\end{lstlisting}

This will create a partition of the physical disk used for the guest VM to use instead and attach it to the guest.

\subsection {Securing Network Storage Devices}

Although a performance hit may be incurred, a storage device using the network may be a more sustainable and secure
solution if possible. This would allow for easier storage backups and failure tolerance. An extra consideration from
this perspective is to ensure that the data is encrypted while transferred and at rest and the encryption keys are
properly stored.

We can verify that the network storage devices are secure with a similar script to the physical devices. We want to
verify that the data is encrypted during transfer and at rest, and that the encryption keys are not available to the host machine.

\begin{lstlisting}[style=AMMA, language=bash]
echo "Hello, World"
\end{lstlisting}

If encryption is not set up, we can do so by running

\begin{lstlisting}[style=AMMA, language=bash]
echo "Hello, World"
\end{lstlisting}

\subsection{Disable Access to Physical Devices}

A similar issue to granting a guest an entire storage device is granting a guest access to any necessary hardware such
as USB devices. Depending on system requirements this may be necessary for a special system, but this would still
increase the attack surface significantly and should be avoided.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=1\textwidth]{./fig/KVM IO.drawio.png}
    \end{center}
    \caption{Device driver code required for external devices contributes significantly to the attack surface of a hypervisor.}
    \label{fig5}
\end{figure}

Since this paper focuses on security considerations in cloud environments, we will not discuess the steps needed to
secure physical devices and instead study the steps taken to use KVM without support for these devices.

In a public cloud, there is no need to enable any external devices unless it is a business requirement. The vast
majority of cloud computing will only require access to block storage and the network. There exist entire virtual
machine monitors built around the concept of stripping the VMM down to only what is necessary for cloud computing.

\subsubsection{Firecracker and gVisor}

Firecracker, developed by Amazon for AWS Lambda, is probably the most widely-used example of this today. This VMM
replaces QEMU and reduces the attack surface significantly by omitting any device drivers for external devices. gVisor,
developed by Google for its App Engine product on GCP, is an example of a similar software which accomplishes the same
goal in a different way. gVisor is more similar to container software as it does not utilize KVM or virtualization.
However, it does create a sandboxed execution environment by implementing a 'user-space kernel' which provides a layer
between each guest container and the linux kernel itself through which the guest's system calls are made.

Both of these approaches isolate the guest code from the host's kernel, but in doing so execute far more kernel code
than a simple linux process on the host machine would~\cite{firecracker_gvisor}. This means that although the host
will now be more protected from traditional malware, there will be more opportunities for an unknown kernel bug to impact
the host and steps should be taken accordingly to protect the host.

\subsubsection{KVM without QEMU}

QEMU is a widely-known and used frontend for KVM virtual machines. However, QEMU has been the source of many bugs which could
lead to remote code execution and therefore may not be the most secure choice in a production environment~\cite{qemu_escape}.
It attempts to emulate as many device drivers as possible so that a virtual machine can function as similarly to a native machine as
possible. This does greatly increase the attack surface since the majority of these devices would not be needed in a
server environment.

Firecracker, developed for AWS, addresses these issues by only supporting the bare minimum IO devices needed for a server.
This means that it does not even implement a full keyboard driver. Firecracker is purely optimized for server workloads,
especially serverless workloads as it boasts a < 125ms startup time~\cite{firecracker_home}. Other than its lack of
support for additional IO devices, it works very similar to a QEMU virtual machine. It can be controlled by an HTTP api
or via command line. To start a simple Firecracker VM, only a Linux kernel and a file system image is required after
firecracker is installed on a system with KVM enabled.

\begin{lstlisting}[style=AMMA, language=bash]
cat <<EOF > ./vmconfig.json
{
  "boot-source": {
    "kernel_image_path": "./vmlinux.bin",
  },
  "drives": [
    {
      "drive_id": "rootfs",
      "path_on_host": "./rootfs.ext4",
      "is_root_device": true,
      "is_read_only": false
    }
  ],
}
EOF
# start firecracker
sudo firecracker --no-api --config-file ./vmconfig.json
\end{lstlisting}

\subsubsection{LXC without system calls}

Google's gVisor takes an approach to sandboxing more similar to containers than VM's. gVisor functions by utilizing
LXC containerization but handles the container's system calls itself instead of passing them directly to the kernel.
This means that gVisor's 'Sentry' component is essentially the VM for the container through which they interact with
the host machine~\cite{gvisor_docs}. The use of gVisor with containers instead of a virtual machine will lead to greater
performance than a fully virtualized setup through KVM while minimizing the security drawbacks of a container-only system.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{./fig/gVisor.drawio.png}
    \end{center}
    \caption{gVisor functions as a layer between LXC and the Linux kernel.}
    \label{fig6}
\end{figure}


\subsection{Secure VM Image Storage}

VM images should be stored in a single, secure location. The images should not be accessible by any users other than the
ones who must read or modify them. Images stored at rest are a target for a malicious actor since injecting malicious
code can allow an attack to remain ongoing and undetected for a long period of time similar to a traditional boot
sector virus. This virus would be recreated every time a new VM is created, allowing the attacker to steal information
from an organization for an ongoing period. As long as it is not publicly readable or writable and the relevant AppArmor
profiles are set up, the images can be considered secured. We can check this along with the other file permission checks.

While it is possible to store images in separate locations if both locations are secure, it is much more manageable and
less error-prone to store them in a single location. Storage of VM images in other locations should be disallowed and
enforced by setting up an AppArmor profile which prevents KVM from using images in any other location. We can check
the security policies on libvirtd

\begin{lstlisting}[style=AMMA, language=bash]
sudo cat /etc/apparmor.d/usr.sbin.libvirtd
\end{lstlisting}

Additionally, we can check whether we have a policy in place to prevent use of other images with

\begin{lstlisting}[style=AMMA, language=bash]
echo "Hello, World"
\end{lstlisting}

This issue is fairly straightforward to resolve. We will first move all images into a single location. Then, we can
reduce the permissions on the files and directories, and finally enforce a new AppArmor profile for KVM.

\begin{lstlisting}[style=AMMA, language=bash]
echo "Hello, World"
\end{lstlisting}

\section{Guest VM Considerations}

As with a KVM host, all procedures to secure a bare metal machine should be followed when configuration a guest VM.
A guest VM will likely not even know it is a VM, so it should be secured in the same way as the host. However, there are
some considerations when configuring the guests that should be taken into account to prevent holes in the security setup.

\subsection{Disable Networking Between Host and Guest VMs}

A guest VM should function as if it was an independent system especially if the VM is in a public cloud. Unless the
requirements state that the VM guests should be able to communicate, there should be no network communication allowed
between the guests. We can check the network settings using virsh

\begin{lstlisting}[style=AMMA, language=bash]
echo "Hello, World"
\end{lstlisting}

If the network configured allows communication between guests which should not have access, we should reassign it. This
can be done without having to restart the system, but could cause issues with any existing iptables or general firewall
rules if the guest is serving network requests.

\begin{lstlisting}[style=AMMA, language=bash]
echo "Hello, World"
\end{lstlisting}

\subsection{Storage Encryption}

A guest OS's storage should be encrypted whether it is stored on a physical disk partition or on a network drive. In
both instances, the physical location of the data would likely be shared with other VM's and could be accessible by
another machine with access to the drive. To prevent any unintentional data leak or leaks from malicious actors, the
storage should always be encrypted whether it is at rest or in transmission, and the encryption keys should be
stored securely.

Any unencrypted data could be read easily during a man-in-the-middle attack if a network drive is used, or directly
from the disk if an attacker has root control of the host. To check whether data will be encrypted by the kernel, we
can examine the xml definition for the storage volume. In the following file, the encryption tag tells libvirt to create
a volume with the given keys

\begin{lstlisting}[style=AMMA, language=xml]
<volume>
  <name>example.luks</name>
  <capacity unit='G'>1</capacity>
  <target>
    <path>/var/lib/libvirt/images/example.luks</path>
    <format type='raw'/>
    <encryption format='luks'>
       <secret type='passphrase' uuid='20755d8a-5ec5-4fd8-94cf-95aa7aea9ba4'/>
       <cipher name='twofish' size='256' mode='cbc' hash='sha256'/>
       <ivgen name='plain64' hash='sha256'/>
    </encryption>
  </target>
</volume>
\end{lstlisting}

We should always be sure to initialize the VM with an encrypted storage device, such as with

\begin{lstlisting}[style=AMMA, language=bash]
virsh attach-disk --config [vm name] [storage xml definition]
\end{lstlisting}

If this is not possible, such as with a VM which has already stored data, then the data must be copied to an encrypted
location and the storage device must be switched. This would not be a trivial operation to accomplish depending on the
role of the VM, and certainly difficult to accomplish with incurring downtime.

\subsection{Hardened VM Images}

Hardened VM images are images which have been pre-configured with a set of security rules in place. Use of this type of
image can prevent many user errors since the image has been officially verified and is shared by many organizations.
This approach is likely preferable to using a stock VM image such as the one provided by Ubuntu or RedHat but should
still be supplemented by a security scan to ensure that the image's best practices are not out of date or misconfigured.

This approach might still not be possible when working with an existing system such as a long-running guest VM.
There also may not be an image provided which meets the requirements for an organization. If the requirements of the
project dictate that a new hardened image can't be used for a guest VM, then it is possible to 'harden' the existing
system and create an image of it for the organization's own use.

Once a system has been configured to meet some security standard, we can create a new VM image based on the system
using

\begin{lstlisting}[style=AMMA, language=bash]
virt-install \
  --name [vm name] \
  --cdrom [path to iso] \
  --os-variant [os type]
\end{lstlisting}

This will create a copy of the current system when installed, so a blank version of the system should be used to create
the ISO. If it is not possible to get a 'blank' version, more complex steps would be required to only copy needed files
to the ISO.
